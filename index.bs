<pre class='metadata'>
Title: Web Neural Network API
Shortname: webnn
Level: 1
Status: CG-DRAFT
Group: webml
URL: https://webmachinelearning.github.io/webnn/
Editor: Ningxin Hu 68202, Intel Corporation https://intel.com
Editor: Chai Chaoweeraprasit 120203, Microsoft Corporation https://microsoft.com
Abstract: This document describes a dedicated low-level API for neural network inference hardware acceleration.
Repository: https://github.com/webmachinelearning/webnn
!Explainer: <a href="https://github.com/webmachinelearning/webnn/blob/master/explainer.md">explainer.md</a>
Markup Shorthands: markdown yes
Markup Shorthands: dfn yes
Markup Shorthands: idl yes
Markup Shorthands: css no
Logo: https://webmachinelearning.github.io/webmachinelearning-logo.png
</pre>

Introduction {#intro}
=====================

We're working on this section. Meanwhile, please take a look at the <a href="https://github.com/webmachinelearning/webnn/blob/master/explainer.md">explainer</a>.

Use cases {#usecases}
=====================

## Application Use Cases ## {#usecases-application}

This section illustrates application-level use cases for neural network
inference hardware acceleration. All applications in those use cases can be
built on top of pre-trained deep neural network (DNN) models.

### Person Detection ### {#usecase-person-detection}

A user opens a web-based video conferencing application, but she temporarily
leaves from her room. The application is watching whether she is in front of her
PC by using object detection (for example, using object detection approaches
such as [[SSD]] or [[YOLO]] that use a single DNN) to detect regions in a camera
input frame that include persons.

When she comes back, the application automatically detects her and notifies
other online users that she is active now.

### Semantic Segmentation ### {#usecase-segmentation}

A user joins a teleconference via a web-based video conferencing application at
her desk since no meeting room in her office is available. During the
teleconference, she does not wish that her room and people in the background are
visible. To protect the privacy of the other people and the surroundings, the
application runs a machine learning model such as [[DeepLabv3+]] or
[[MaskR-CNN]] to semantically split an image into segments and replaces
segments that represent other people and background with another picture.

### Skeleton Detection ### {#usecase-skeleton-detection}

A web-based video conferencing application tracks a pose of user's skeleton by
running a machine learning model, which allows for real-time human pose
estimation, such as [[PoseNet]] to recognize her gesture and body language. When
she raises her hand, her microphone is automatically unmuted and she can start
speaking on the teleconference.

### Face Recognition ### {#usecase-face-recognition}

There are multiple people in the conference room and they join an online meeting
using a web-based video conferencing application. The application detects faces
of participants by using object detection (for example, using object detection
approaches such as [[SSD]]) and checks whether each face was present at the
previous meeting or not by running a machine learning model such as [[FaceNet]],
which verifies whether two faces would be identical or not.

### Facial Landmark Detection ### {#usecase-facial-landmarks}

A user wants to find new glasses that beautifully fits her on an online glasses
store. The online store offers web-based try-on simulator that runs a machine
learning model such as Face Alignment Network [[FAN]] to detect facial landmarks
like eyes, nose, mouth, etc. When she chooses a pair of glasses, the simulator
properly render the selected glasses on the detected position of eyes on her
facial image.

### Style Transfer ### {#usecase-style-transfer}

A user is looking for cosmetics on an online store and wondering which color may
fit her face. The online store shows sample facial makeup images of cosmetics,
and offers makeup simulator that runs a machine learning model like
[[ContextualLoss]] or [[PairedCycleGAN]] to transfer the makeup style of the
sample makeup image to her facial image. She can check how the selected makeup
looks like on her face by the simulator.

### Super Resolution ### {#usecase-super-resolution}

A web-based video conferencing is receiving a video stream from its peer, but
the resolution of the video becomes lower due to network congestion. To prevent
degradation of the perceived video quality, the application runs a machine
learning model for super-resolution such as [[SRGAN]] to generate
higher-resolution video frames.

### Image Captioning ### {#usecase-image-captioning}

For better accessibility, a web-based presentation application provides
automatic image captioning by running a machine learning model such as
[[im2txt]] which predicts explanatory words of the presentation slides.

### Machine Translation ### {#usecase-translation}

Multiple people from various countries are talking via a web-based real-time
text chat application. The application translates their conversation by using a
machine learning model such as [[GNMT]] or [[OpenNMT]], which translates every
text into different language.

### Emotion Analysis ### {#usecase-emotion-analysis}

A user is talking to her friend via a web-based real-time text chat application,
and she is wondering how the friend feels because she cannot see the friend's
face. The application analyses the friend's emotion by using a machine learning
model such as [[DeepMoji]], which infers emotion from input texts, and displays
an emoji that represents the estimated emotion.

### Video Summarization ### {#usecase-video-summalization}

A web-based video conferencing application records received video streams, and
it needs to reduce recorded video data to be stored. The application generates
the short version of the recorded video by using a machine learning model for
video summarization such as [[Video-Summarization-with-LSTM]].

### Noise Suppression ### {#usecase-noise-suppression}

A web-based video conferencing application records received audio streams, but 
usually the background noise is everywhere. The application leverages real-time 
noise suppression using Recurrent Neural Network such as [[RNNoise]] for 
suppressing background dynamic noise like baby cry or dog barking to improve 
audio experiences in video conferences.

## Framework Use Cases ## {#usecases-framework}

This section collects framework-level use cases for a dedicated low-level API
for neural network inference hardware acceleration. It is expected that Machine
Learning frameworks will be key consumers of the Web Neural Network API (WebNN
API) and the low-level details exposed through the WebNN API are abstracted out
from typical web developers. However, it is also expected that web developers
with specific interest and competence in Machine Learning will want to interface
with the WebNN API directly instead of a higher-level ML framework.

### Custom Layer ### {#usecase-custom-layer}

A web application developer wants to run a DNN model on the WebNN API. However,
she has found that some of activation functions like [[LeakyReLU]], [[ELU]],
etc. are not included in the WebNN API. To address this issue, she constructs
custom layers of the additional activation functions on top of the WebNN API.
Note that the scope of custom layers may include convolution, normalization,
etc. as well as activation.

### Network Concatenation ### {#usecase-network-concat}

A web application uses a DNN model, and its model data of upper convolutional
layers and lower fully-connected layers are stored in separate files, since
model data of the fully-connected layers are periodically updated due to fine
tuning at the server side.

Therefore, the application downloads both partial model files at first and
concatenates them into a single model. When the model is updated, the
application downloads fine-tuned part of the model and replace only the
fully-connected layers with it.

### Performance Adaptation ### {#usecase-perf-adapt}

A web application developer has a concern about performance of her DNN model on
mobile devices. She has confirmed that it may run too slow on mobile devices
which do not have GPU acceleration. To address this issue, her web application
refers to the WebNN API to confirm whether acceleration is available or not, so
that the application can display the warning for devices without acceleration.

After several weeks, she has developed a tiny DNN model that can even run on
CPU. In order to accommodate CPU execution, she modifies the application
so that the application loads the tiny model in the case of CPU-only devices.

API {#api}
=====================

## Navigator ## {#api-navigator}
<script type=idl>
partial interface Navigator {
  readonly attribute ML ml;
};
</script>

## ML ## {#api-ml}
<script type=idl>
interface ML {
  NeuralNetworkContext getNeuralNetworkContext();
};
</script>

## OperandDescriptor ## {#api-operanddescriptor}
<script type=idl>
enum OperandLayout {
  "nchw",
  "nhwc"
};

enum OperandType {
  "float32",
  "float16",
  "int32",
  "uint32",
  "tensor-float32",
  "tensor-float16",
  "tensor-int32",
  "tensor-quant8-asymm"
};

dictionary OperandDescriptor {
  // The operand type.
  required OperandType type;

  // The dimensions field is only required for tensor operands.
  // The negative value means an unknown dimension.
  sequence<long> dimensions;

  // The following two fields are only required for quantized operand.
  // scale: an non-negative floating point value
  // zeroPoint: an integer, in range [0, 255]
  // The real value is (value - zeroPoint) * scale
  float scale;
  long zeroPoint;
};
</script>

## Operand ## {#api-operand}
<script type=idl>
interface Operand {};
</script>

## NeuralNetworkContext ## {#api-neuralnetworkcontext}

The {{NeuralNetworkContext}} defines a set of operations derived from the first-wave models [[Models]] that address identified [[#usecases]].

<script type=idl>
typedef double number;

dictionary NamedOperand {
  required DOMString name;
  required Operand operand;
};

interface NeuralNetworkContext {
  // Create an Operand object that represents a model input.
  Operand input(DOMString name, OperandDescriptor desc);

  // Create an Operand object that represents a model constant.
  Operand constant(OperandDescriptor desc, ArrayBufferView value);

  // Create a single-value tensor from the specified number of the specified type.
  Operand constant(number value, optional OperandType type = "float32");

  // Create a Model object by identifying output operands.
  Promise<Model> createModel(sequence<NamedOperand> outputs);
};
</script>

### batchNormalization ### {#api-neuralnetworkcontext-batchnorm}
Normalize the tensor values across dimensions in a batch through a specialized [[BatchNorm]] [transform](https://en.wikipedia.org/wiki/Batch_normalization#Batch_Normalizing_Transform). The *mean* and *variance* tensors are previously calculated during model training pass.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand batchNormalization(Operand input, Operand mean, Operand variance, 
                             optional Operand scale, optional Operand bias, 
                             optional long axis = 1, optional float epsilon = 1e-5);
};
</script>
<div algorithm=batchnorm>
    **Arguments:**
        - *input*: an {{Operand}}. The input N-D tensor.
        - *mean*: an {{Operand}}. The 1-D tensor of the batch mean values 
            whose length is equal to the size of the input dimension denoted by *axis*.
        - *variance*: an {{Operand}}. The 1-D tensor of the batch variance values
            whose length is equal to the size of the input dimension denoted by *axis*.
        - *scale*: an optional {{Operand}}. The 1-D tensor of the scaling values
            whose length is equal to the size of the input dimension denoted by *axis*.
        - *bias*: an optional {{Operand}}. The 1-D tensor of the bias values
            whose length is equal to the size of the input dimension denoted by *axis*.
        - *axis*: an optional {{long}}. The index for the feature dimension of the input shape for which 
            the normalizing batch is defined and that the mean and variance values are computed. 
            When it's not specified, the default value is 1.
        - *epsilon*: an optional {{float}}. The Epsilon value to prevent computational error due to divide-by-zero.
            The default value is 0.00001 when not specified.
        
    **Returns:** an {{Operand}}. The batch normalized N-D tensor of the same shape as the input tensor.

    When *input* is a 4-D tensor of the "nchw" or "nhwc" layout, *axis* should be set to 1 or 3 respectively, 
    to designate the feature dimension of the input tensor.

    <div class="note">
    The behavior of this operation when the input tensor is 4-D of the "nchw" layout can be generically emulated from 
    the usage of other operations as follow. However, user agents typically have a more efficient implementation for it, 
    therefore its usage is encouraged from the performance standpoint.
    <pre highlight="js">
    let shape = [1,-1,1,1];
    return nn.add(
        nn.mul(
          nn.reshape(scale, shape),
          nn.div(
            nn.sub(input, reshape(mean, shape)),
            nn.sqrt(nn.add(nn.reshape(variance, shape), nn.constant(epsilon)))
            )
          ),
        nn.reshape(bias, shape)
      );
    </pre>
    </div>
</div>

### concat ### {#api-neuralnetworkcontext-concat}
Concatenates the input tensors along a given axis.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand concat(sequence<Operand> inputs, long axis);
};
</script>
<div algorithm=concat>
    **Arguments:**
        - *inputs*: a sequence of {{Operand}}. All input tensors must have the
            same shape, except for the size of the dimension to concatenate on.
        - *axis*: a {{long}}. The axis that the inputs concatenate along, with
            the value in the interval [0, N) where N is the rank of all the
            inputs.
        
    **Returns:** an {{Operand}}. The concatenated tensor of all the inputs along
    the *axis*. The output tensor has the same shape except on the dimension
    that all the inputs concatenated along. The size of that dimension is
    computed as the sum of all the input sizes of the same dimension.
</div>

### conv2d ### {#api-neuralnetworkcontext-conv2d}
Compute a 2-D convolution given 4-D input and filter tensors
<script type=idl>
partial interface NeuralNetworkContext {
  Operand conv2d(Operand input, Operand filter,
                 optional sequence<long> padding, optional sequence<long> strides, 
                 optional sequence<long> dilations, optional long groups = 1,
                 optional OperandLayout layout = "nchw");
};
</script>
<div algorithm=conv2d>
    **Arguments:**
        - *input*: an {{Operand}}. The input 4-D tensor. The logical shape
            is interpreted according to the value of *layout*.
        - *filter*: an {{Operand}}. The filter 4-D tensor. The logical shape is
            interpreted according to the value of *layout* and *groups*.
        - *padding*: an optional sequence of {{long}} of length 4. The padding for the
            beginning and ending along each spatial dimension of *input*,
            [beginning_height, ending_height, beginning_width, ending_width]. 
            If not present, the values are assumed to be [0,0,0,0].
        - *strides*: an optional sequence of {{long}} of length 2. The stride of the
            sliding window for each spatial dimension of *input*, [stride_height, stride_width]. 
            If not present, the values are assumed to be [1,1].
        - *dilations*: an optional sequence of {{long}} of length 2. The dilation factor
            for each spatial dimension of *input*, [dilation_height, dilation_width]. 
            If not present, the values are assumed to be [1,1].
        - *groups*: an optional {{long}}. The number of groups that input
            channels and output channels are divided into, default to 1.
        - *layout*: an optional {{OperandLayout}} with value as "nchw" or
            "nhwc". The default value is "nchw". This argument specifies the
            layout format of the input, output and filter tensor.
            Specifically,

            "nchw":
                - input tensor: [batches, input_channels, height, width]
                - filter tensor: [output_channels, input_channels/groups,
                    height, width]
                - output tensor: [batches, output_channels, height, width]
            "nhwc":
                - input tensor: [batches, height, width, input_channels]
                - filter tensor: [height, width, input_channels/groups,
                    output_channels]
                - output tensor: [batches, height, width, output_channels]

    **Returns:** an {{Operand}}. The output 4-D tensor that contains the
    result of the convolution. The logical shape is interpreted according to the
    value of *layout*.

    <div class="note">
    A *depthwise* conv2d operation is a variant of grouped conv2d, used in
    models like the MobileNet, where the *groups* = input_channels =
    output_channels and the shape of filter tensor is [groups, 1, height, width]
    for "nchw" layout or [height, width, 1, groups] for "nhwc" layout.
    </div>
</div>

### element-wise binary operations ### {#api-neuralnetworkcontext-binary}
Compute the element-wise binary addition, subtraction, multiplication, division,
maximum and minimum of the two input tensors.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand add(Operand a, Operand b);
  Operand sub(Operand a, Operand b);
  Operand mul(Operand a, Operand b);
  Operand div(Operand a, Operand b);
  Operand max(Operand a, Operand b);
  Operand min(Operand a, Operand b);
};
</script>
<div algorithm=binary>
    **Arguments:**
        - *a*: an {{Operand}}. The first input tensor.
        - *b*: an {{Operand}}. The second input tensor.

    **Returns:** an {{Operand}}. The output tensor that contains the result of
    element-wise binary operation of the two input tensors.

    The element-wise binary operation will be broadcasted according to
    [[!numpy-broadcasting-rule]]. The rank of the output tensor is the maximum
    rank of the input tensors. For each dimension of the output tensor, its size
    is the maximum size along that dimension of the input tensors.
</div>

### element-wise unary operations ### {#api-neuralnetworkcontext-unary}
Compute the element-wise unary operation for input tensor.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand abs(Operand x);
  Operand ceil(Operand x);
  Operand cos(Operand x);
  Operand exp(Operand x);
  Operand floor(Operand x);
  Operand log(Operand x);
  Operand neg(Operand x);
  Operand sigmoid(Operand x);
  Operand sin(Operand x);
  Operand sqrt(Operand x);
  Operand tan(Operand x);
  Operand tanh(Operand x);
};
</script>
<div algorithm=unary>
    **Arguments:**
        - *x*: an {{Operand}}. The input tensor.

    **Returns:** an {{Operand}}. The output tensor that contains the result of
    element-wise unary operation of the input tensor. The shape of the output
    tensor is the same as the shape of input tensor.

    **Operation types:**
        - *abs*: Compute the absolute value of the input tensor, element-wise.
        - *ceil*: Compute the ceiling of the input tensor, element-wise.
        - *cos*: Compute the cosine of the input tensor, element-wise.
        - *exp*: Compute the exponential of the input tensor, element-wise.
        - *floor*: Compute the floor of the input tensor, element-wise.
        - *log*: Compute the natural logarithm of the input tensor, element-wise.
        - *neg*: Compute the numerical negative value of the input tensor, element-wise.
        - *sigmoid*: Compute the sigmoid function of the input tensor, element-wise.
        - *sin*: Compute the sine of the input tensor, element-wise.
        - *sqrt*: Compute the square root of the input tensor, element-wise.
        - *tan*: Compute the tangent of the input tensor, element-wise.
        - *tanh*: Compute the hyperbolic tangent of the input tensor, element-wise.
</div>

### gemm ### {#api-neuralnetworkcontext-gemm}
Calculate the [general matrix multiplication of the Basic Linear Algebra Subprograms](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3). The calculation follows the expression `alpha * A * B + beta * C`, where `A`, `B`, and `C` are matrices, and `A` and `B` may optionally be transposed prior to the calculation.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand gemm(Operand a, Operand b, optional Operand c, 
               optional float alpha = 1.0, optional float beta = 1.0, 
               optional boolean aTranspose = false, optional boolean bTranspose = false);
};
</script>
<div algorithm=gemm>
    **Arguments:**
        - *a*: an {{Operand}}. The first input 2-D tensor.
        - *b*: an {{Operand}}. The second input 2-D tensor.
        - *c*: an optional {{Operand}}. The third input 2-D tensor.
        - *alpha*: an optional {{float}} scalar multiplier for the first input, default to 1.0.
        - *beta*: an optional {{float}} scalar multiplier for the third input, default to 1.0.
        - *aTranspose*: an optional {{boolean}} indicating if the first input should be transposed prior to calculating the output, default to false.
        - *bTranspose*: an optional {{boolean}} indicating if the second input should be transposed prior to calculating the output, default to false.

    **Returns:** an {{Operand}}. The output 2-D tensor that contains the calculated product of all the inputs.

    <div class="note">
    The behavior of this operation can be generically emulated from the usage of other operations as follow. However, user agents typically have a more efficient implementation for it, therefore its usage is encouraged from the performance standpoint.
    <pre highlight="js">
    if (aTranspose)
      a = nn.transpose(a);

    if (bTranspose)
      b = nn.transpose(b);

    let ab = nn.matmul(nn.mul(nn.constant(alpha), a), b);
    return (c ? nn.add(ab, nn.mul(nn.constant(beta), c)) : ab);
    </pre>
    </div>
</div>

### gru ### {#api-neuralnetworkcontext-gru}
Gated Recurrent Unit [[GRU]] recurrent network using an update gate and a reset gate to compute the hidden state that rolls into the output across the temporal sequence of the Network
<script type=idl>
enum RecurrentNetworkDirection {
  "forward",
  "backward",
  "both"
};

enum RecurrentNetworkWeightLayout {
  "zrn",  // update-reset-new gate ordering
  "rzn"   // reset-update-new gate ordering
};

enum RecurrentNetworkActivation {
  "relu",
  "sigmoid",
  "tanh"
};

partial interface NeuralNetworkContext {
  sequence<Operand> gru(Operand input, Operand weight, Operand recurrentWeight, 
                        long steps, long hiddenSize,
                        optional Operand bias, optional Operand recurrentBias,
                        optional Operand initialHiddenState,
                        optional boolean resetAfter = true,
                        optional boolean returnSequence = false,
                        optional RecurrentNetworkDirection direction = "forward",
                        optional RecurrentNetworkWeightLayout layout = "zrn",
                        optional sequence<RecurrentNetworkActivation> activations);
};
</script>
<div algorithm=gru>
    **Arguments:**
        - *input*: an {{Operand}}. The input 3-D tensor of shape [steps, batch_size, input_size]. 
        - *weight*: an {{Operand}}. The 3-D input weight tensor of shape [num_directions, 3 * hidden_size, input_size]. The ordering of the weight vectors in the second dimension of the tensor shape is specified according to the *layout* argument.
        - *recurrentWeight*: an {{Operand}}. The 3-D recurrent weight tensor of shape [num_directions, 3 * hidden_size, hidden_size]. The ordering of the weight vectors in the second dimension of the tensor shape is specified according to the *layout* argument.
        - *steps*: a {{long}} scalar. The number of time steps in the recurrent network. The value must be greater than 0.
        - *hiddenSize*: a {{long}} scalar. The value of the third dimension of the cell output tensor shape. It indicates the number of features in the hidden state.
        - *bias*: an optional {{Operand}}. The 2-D input bias tensor of shape [num_directions, 3 * hidden_size]. The ordering of the bias vectors in the second dimension of the tensor shape is specified according to the *layout* argument.
        - *recurrentBias*: an optional {{Operand}}. The 2-D recurrent bias tensor of shape [num_directions, 3 * hidden_size]. The ordering of the bias vectors in the second dimension of the tensor shape is specified according to the *layout* argument.
        - *initialHiddenState*: an optional {{Operand}}. The 3-D initial hidden state tensor of shape [num_directions, batch_size, hidden_size]. When not specified, it's assumed to be a tensor filled with zero.
        - *resetAfter*: an optional {{boolean}} indicating whether to apply the reset gate after or before matrix multilication. Default to true.
        - *returnSequence*: an optional {{boolean}} indicating whether to also return the entire sequence with every cell outputs from each time step in it in addition to the cell output of the last time step. Default to false.
        - *direction*: an optional {{RecurrentNetworkDirection}}. The processing direction of the input sequence. When set to *"both"*, the size of the first dimension of the weight and the bias tensor shapes must be 2, and the input is processed in both directions.
        - *layout*: an optional {{RecurrentNetworkWeightLayout}}. The ordering of the weight and bias vectors for the internal gates of GRU, specifically the *update (z)*, *reset (r)*, and *new (n)* gate, as indicated in the second dimension of the weight and bias tensor shape. When not specified, the default layout is *"zrn"*.
        - *activations*: an optional sequence of {{RecurrentNetworkActivation}}. A pair of activation functions with the first function used for the update and reset gate, and the second used for the new gate. When not specified, it's assumed to be the sigmoid ("sigmoid") and the hyperbolic tangent ("tanh") function respectively.

    **Returns:** a sequence of {{Operand}}. The first element of the sequence is a 3-D tensor of shape [num_directions, batch_size, hidden_size], the cell output from the last time step of the network. Additionally, if *returnSequence* is set to true, the second element is the 4-D output tensor of shape [steps, num_directions, batch_size, hidden_size] containing every cell outputs from each time step in the temporal sequence.

    <div class="note">
    The behavior of this operation with default argument values can be generically emulated from the usage of other operations as follow. However, user agents typically have a more efficient implementation for it, therefore its usage is encouraged from the performance standpoint.
    <pre highlight="js">
    const numDirections = (direction == "both" ? 2 : 1);
    let hiddenState = initialHiddenState;

    if (!hiddenState) {
      const desc = { type: 'tensor-float32', dimensions: [numDirections, 1, hiddenSize] };
      const totalSize = numDirections * hiddenSize;
      hiddenState = nn.constant(desc, new Float32Array(totalSize).fill(0));
    }

    let sequence = null;
    let cellWeight = [];
    let cellRecurrentWeight = [];
    let cellBias = [];
    let cellRecurrentBias = [];

    for (let slot = 0; slot < numDirections; ++slot) {
      cellWeight.push(nn.squeeze(nn.slice(weight, [slot, 0, 0], [1, -1, -1])), [0]);
      cellRecurrentWeight.push(nn.squeeze(nn.slice(recurrentWeight, [slot, 0, 0], [1, -1, -1])), [0]);
      cellBias.push(bias ? (nn.squeeze(nn.slice(bias, [slot, 0], [1, -1]), [0])) : null);
      cellRecurrentBias.push(recurrentBias ? (nn.squeeze(nn.slice(recurrentBias, [slot, 0], [1, -1]), [0])) : null);
    }

    for (let step = 0; step < steps; ++step) {
      let cellHidden = [];
      let cellOutput = null;

      for (let slot = 0; slot < numDirections; ++slot) {
        cellHidden.push(nn.squeeze(nn.slice(hiddenState, [slot, 0, 0], [1, -1, -1]), [0]));
      }

      for (let slot = 0; slot < numDirections; ++slot) {
        let slice = (slot == 1 || direction == "backward" ? steps - step - 1 : step);
        let cellInput = nn.squeeze(nn.slice(input, [slice, 0, 0], [1, -1, -1]), [0]);

        let result = nn.gruCell(
          cellInput, cellWeight[slot], cellRecurrentWeight[slot], cellHidden[slot], 
          hiddenSize, cellBias[slot], cellRecurrentBias[slot], resetAfter, layout, activations);

        cellOutput = (cellOutput ? nn.concat([cellOutput, result], 0) : result);
      }

      hiddenState = cellOutput;

      if (returnSequence) {
        cellOutput = nn.reshape(cellOutput, [1, numDirections, -1, hiddenSize]);
        sequence = (sequence ? nn.concat([sequence, cellOutput], 0) : cellOutput);
      }
    }

    return (sequence ? [hiddenState, sequence] : [hiddenState]);
    </pre>
    </div>
</div>

### gruCell ### {#api-neuralnetworkcontext-grucell}
A single time step of the Gated Recurrent Unit [[GRU]] recurrent network using an update gate and a reset gate to compute the hidden state that rolls into the output across the temporal sequence of a recurrent network.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand gruCell(Operand input, Operand weight,
                  Operand recurrentWeight, Operand hiddenState, long hiddenSize, 
                  optional Operand bias, optional Operand recurrentBias,
                  optional boolean resetAfter = true,
                  optional RecurrentNetworkWeightLayout layout = "zrn",
                  optional sequence<RecurrentNetworkActivation> activations);
};
</script>
<div algorithm=grucell>
    **Arguments:**
        - *input*: an {{Operand}}. The input 2-D tensor of shape [batch_size, input_size]. 
        - *weight*: an {{Operand}}. The 2-D input weight tensor of shape [3 * hidden_size, input_size]. The ordering of the weight vectors in the first dimension of the tensor shape is specified according to the *layout* argument.
        - *recurrentWeight*: an {{Operand}}. The 2-D recurrent weight tensor of shape [3 * hidden_size, hidden_size]. The ordering of the weight vectors in the first dimension of the tensor shape is specified according to the *layout* argument.
        - *hiddenState*: an {{Operand}}. The 2-D input hidden state tensor of shape [batch_size, hidden_size].
        - *hiddenSize*: a {{long}} scalar. The value of the second dimension of the output tensor shape. It indicates the number of features in the hidden state.
        - *bias*: an optional {{Operand}}. The 1-D input bias tensor of shape [3 * hidden_size]. The ordering of the bias vectors in the first dimension of the tensor shape is specified according to the *layout* argument.
        - *recurrentBias*: an optional {{Operand}}. The 1-D recurrent bias tensor of shape [3 * hidden_size]. The ordering of the bias vectors in the first dimension of the tensor shape is specified according to the *layout* argument.
        - *resetAfter*: an optional {{boolean}} indicating whether to apply the reset gate after or before matrix multilication. Default to true.
        - *layout*: an optional {{RecurrentNetworkWeightLayout}}. The ordering of the weight and bias vectors for the internal gates of GRU, specifically the *update (z)*, *reset (r)*, and *new (n)* gate, as indicated in the first dimension of the weight and bias tensor shapes. When not specified, the default layout is *"zrn"*.
        - *activations*: an optional sequence of {{RecurrentNetworkActivation}}. A pair of activation functions with the first function used for the update and reset gate, and the second used for the new gate. When not specified, it's default to the sigmoid ("sigmoid") and the hyperbolic tangent ("tanh") function respectively.

    **Returns:** an {{Operand}}. The 2-D tensor of shape [batch_size, hidden_size], the cell output hidden state of a single time step of the recurrent network.

    <div class="note">
    The behavior of this operation with default argument values can be generically emulated from the usage of other operations as follow. However, user agents typically have a more efficient implementation for it, therefore its usage is encouraged from the performance standpoint.
    <pre highlight="js">
    const one = nn.constant(1);
    const zero = nn.constant(0);

    // update gate
    let z = nn.sigmoid(
      nn.add(
        nn.add(
          (bias ? nn.slice(bias, [0], [hiddenSize]) : zero), 
          (recurrentBias ? nn.slice(recurrentBias, [0], [hiddenSize]) : zero)
          ),
        nn.add(
          nn.matmul(
            input, 
            nn.transpose(nn.slice(weight, [0, 0], [hiddenSize, -1]))
            ),
          nn.matmul(
            hiddenState,
            nn.transpose(nn.slice(recurrentweight, [0, 0], [hiddenSize, -1]))
            )
          )
        )
      );

    // reset gate
    let r = nn.sigmoid(
      nn.add(
        nn.add(
          (bias ? nn.slice(bias, [hiddenSize], [hiddenSize]) : zero),
          (recurrentBias ? nn.slice(recurrentBias, [hiddenSize], [hiddenSize]) : zero)
          ),
        nn.add(
          nn.matmul(
            input, 
            nn.transpose(nn.slice(weight, [hiddenSize, 0], [hiddenSize, -1]))
            ),
          nn.matmul(
            hiddenState, 
            nn.transpose(nn.slice(recurrentweight, [hiddenSize, 0], [hiddenSize, -1]))
            )
          )
        )
      );

    // new gate
    let n;
    if (resetAfter) {
      n = nn.tanh(
        nn.add(
          (bias ? nn.slice(bias, [2 * hiddenSize], [hiddenSize]) : zero),
          nn.add(
            nn.matmul(
              input, 
              nn.transpose(nn.slice(weight, [2 * hiddenSize, 0], [hiddenSize, -1]))
              ),
            nn.mul(
              r,
              nn.add(
                (recurrentBias ? nn.slice(recurrentBias, [2 * hiddenSize], [hiddenSize]) : zero),
                nn.matmul(
                  hiddenState, 
                  nn.transpose(nn.slice(recurrentweight, [2 * hiddenSize, 0], [hiddenSize, -1]))
                  )
                )
              )
            )
          )
        );
    }
    else {
      n = nn.tanh(
        nn.add(
          nn.add(
            (bias ? nn.slice(bias, [2 * hiddenSize], [hiddenSize]) : zero),
            (recurrentBias ? nn.slice(recurrentBias, [2 * hiddenSize], [hiddenSize]) : zero)
            ),
          nn.add(
            nn.matmul(
              input, 
              nn.transpose(nn.slice(weight, [2 * hiddenSize, 0], [hiddenSize, -1]))
              ),
            nn.matmul(
              nn.mul(r, hiddenState),
              nn.transpose(nn.slice(recurrentweight, [2 * hiddenSize, 0], [hiddenSize, -1]))
              )
            )
          )
        );
    }

    // compute the new hidden state
    return nn.add(nn.mul(z, hiddenState), nn.mul(n, nn.sub(one, z)));
    </pre>
    </div>
</div>

### leakyRelu ### {#api-neuralnetworkcontext-leakyrelu}
<script type=idl>
partial interface NeuralNetworkContext {
  Operand leakyRelu(Operand x, optional float alpha = 0.01);
};
</script>
<div algorithm=leakyrelu>
    **Arguments:**
        - *x*: an {{Operand}}. The input tensor.
        - *alpha*: an optional {{float}} scalar multiplier, default to 0.01.

    **Returns:** an {{Operand}}. The output tensor of the same shape as *x*.

    Calculate the <a
    href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Leaky_ReLU">
    leaky version of rectified linear function</a> on the input tensor
    element-wise. The calculation follows the expression `max(0, x) + alpha âˆ—
    min(0, x)`.

    <div class="note">
    The behavior of this operation can be generically emulated from the usage of
    other operations as follow. However, user agents typically have a more
    efficient implementation for it, therefore its usage is encouraged from the
    performance standpoint.
    <pre highlight="js">
    return nn.add(nn.max(nn.constant(0), x),
                  nn.mul(nn.constant(alpha), nn.min(nn.constant(0), x)));
    </pre>
    </div>
</div>

### matmul ### {#api-neuralnetworkcontext-matmul}
Compute the matrix product of two input tensors.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand matmul(Operand a, Operand b);
};
</script>

<div algorithm=matmul>
    **Arguments:**
        - *a*: an {{Operand}}. The first input N-D tensor.
        - *b*: an {{Operand}}. The second input N-D tensor.

    **Returns:** an {{Operand}}. The output N-D tensor that contains the matrix
    product of two input tensors.

    Compute the matrix product of two input tensors. It behaves as following:
        - If both *a* and *b* are 2-D, they are multiplied like conventional
            matrices and produce a 2-D tensor as the output.
        - If either *a* or *b* is N-D, N > 2, it is treated as a stack of
            matrices with dimensions corresponding to the last two indices. The
            matrix multiplication will be broadcasted accordingly by following
            [[!numpy-broadcasting-rule]]. The output is a N-D tensor whose rank
            is the maximum rank of the input tensors. For each dimension, except
            the last two, of the output tensor, its size is the maximum size
            along that dimension of the input tensors.
        - If *a* is 1-D, it is converted to a 2-D tensor by prepending a 1 to
            its dimensions.
        - If *b* is 1-D, it is converted to a 2-D tensor by by appending a 1 to
            its dimensions.
        - If both *a* and *b* are 1-D, the operation is a vector dot-product,
            which produces a scalar output.
</div>

### pooling operations ### {#api-neuralnetworkcontext-pool2d}
Compute a *mean*, *L2 norm*, or *max* reduction operation across all the elements within the moving window over the input tensor. See the description of each type of reduction in [[#api-neuralnetworkcontext-reduce]].
<script type=idl>
partial interface NeuralNetworkContext {
  Operand averagePool2d(Operand input, optional sequence<long> windowDimensions,
                        optional sequence<long> padding, optional sequence<long> strides,
                        optional sequence<long> dilations, optional OperandLayout layout = "nchw");
  Operand l2Pool2d(Operand input, optional sequence<long> windowDimensions,
                  optional sequence<long> padding, optional sequence<long> strides,
                  optional sequence<long> dilations, optional OperandLayout layout = "nchw");
  Operand maxPool2d(Operand input, optional sequence<long> windowDimensions,
                    optional sequence<long> padding, optional sequence<long> strides,
                    optional sequence<long> dilations, optional OperandLayout layout = "nchw");
};
</script>
<div algorithm=pool2d>
    **Arguments:**
        - *input*: an {{Operand}}. The input 4-D tensor. The logical shape
            is interpreted according to the value of *layout*.
        - *windowDimensions*: an optional sequence of {{long}} of length 2. The dimensions of the sliding window,
            [window_height, window_width]. If not present, the window dimensions are assumed to be the height  
            and width dimensions of the input shape. 
        - *padding*: an optional sequence of {{long}} of length 4. The padding for the
            beginning and ending along each spatial dimension of *input*,
            [beginning_height, ending_height, beginning_width, ending_width].
            If not present, the values are assumed to be [0,0,0,0].
        - *strides*: an optional sequence of {{long}} of length 2. The stride of the
            sliding window for each spatial dimension of *input*,
            [stride_height, stride_width]. If not present, the values are assumed to be [1,1].
        - *dilations*: an optional sequence of {{long}} of length 2. The dilation factor
            for each spatial dimension of *input*, [dilation_height, dilation_width].
            If not present, the values are assumed to be [1,1].
        - *layout*: an optional {{OperandLayout}} with value as "nchw" or
            "nhwc". The default value is "nchw". This argument specifies the
            layout format of the input and output.

            "nchw":
                - input tensor: [batches, channels, height, width]
                - output tensor: [batches, channels, height, width]
            "nhwc":
                - input tensor: [batches, height, width, channels]
                - output tensor: [batches, height, width, channels]

    **Returns:** an {{Operand}}. The output 4-D tensor that contains the
    result of the reduction. The logical shape is interpreted according to the
    value of *layout*.

    <div class="note">
    A *global* pooling operation such as one for the max pooling operation is a variant of pooling where the window dimensions is the spatial dimensions (last two dimensions) of the input shape, as follow.
    <pre highlight="js">
    // 'global' max pooling
    maxPool2d(input);
    </pre>
    </div>
</div>

### reduction operations ### {#api-neuralnetworkcontext-reduce}
Reduce the input along the dimensions given in *axes*.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand reduceL1(Operand input, optional sequence<long> axes = null, optional boolean keepDimensions = false);
  Operand reduceL2(Operand input, optional sequence<long> axes = null, optional boolean keepDimensions = false);
  Operand reduceLogSum(Operand input, optional sequence<long> axes = null, optional boolean keepDimensions = false);
  Operand reduceLogSumExp(Operand input, optional sequence<long> axes = null, optional boolean keepDimensions = false);
  Operand reduceMax(Operand input, optional sequence<long> axes = null, optional boolean keepDimensions = false);
  Operand reduceMean(Operand input, optional sequence<long> axes = null, optional boolean keepDimensions = false);
  Operand reduceMin(Operand input, optional sequence<long> axe = null, optional boolean keepDimensions = false);
  Operand reduceProduct(Operand input, optional sequence<long> axes = null, optional boolean keepDimensions = false);
  Operand reduceSum(Operand input, optional sequence<long> axes = null, optional boolean keepDimensions = false);
  Operand reduceSumSquare(Operand input, optional sequence<long> axes = null, optional boolean keepDimensions = false);
};
</script>
<div algorithm=reduce>
    **Arguments:**
        - *input*: an {{Operand}}. The input tensor.
        - *axes*: an optional sequence of {{long}}. The dimensions to reduce where -1 means the last dimension.
            If not present, all dimensions are reduced.
        - *keepDimensions*: an optional {{boolean}}. If true, retains reduced dimensions with size of 1.
            The default value is false.

    **Returns:** an {{Operand}}. The reduced output tensor.

    **Reduction types:**
        - *L1*: Compute the <a href="https://mathworld.wolfram.com/L1-Norm.html">L1 norm</a> of all the input values along the axes.
        - *L2*: Compute the <a href="https://mathworld.wolfram.com/L2-Norm.html">L2 norm</a> of all the input values along the axes.
        - *LogSum*: Compute the log value of the sum of all the input values along the axes.
        - *LogSumExp*: Compute the log value of the sum of the exponent of all the input values along the axes.
        - *Max*: Compute the maximum value of all the input values along the axes.
        - *Mean*: Compute the average value of all the input values along the axes.
        - *Min*: Compute the minimum value of all the input values along the axes.
        - *Product*: Compute the product of all the input values along the axes.
        - *Sum*: Compute the sum of all the input values along the axes.
        - *SumSquare*: Compute the sum of the square of all the input values along the axes.
</div>

### relu ### {#api-neuralnetworkcontext-relu}
Calculate the [rectified linear](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) function on the input tensor element-wise. The calculation follows the expression `max(0, x)`.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand relu(Operand x);
};
</script>
<div algorithm=relu>
    **Arguments:**
        - *x*: an {{Operand}}. The input tensor.

    **Returns:** an {{Operand}}. The output tensor of the same shape as *x*.

    Calculate the <a
    href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified
    linear function</a> on the input tensor element-wise. The calculation
    follows the expression `max(0, x)`.

    <div class="note">
    The behavior of this operation can be generically emulated from the usage of
    other operations as follow. However, user agents typically have a more
    efficient implementation for it, therefore its usage is encouraged from the
    performance standpoint.
    <pre highlight="js">
    return nn.max(nn.constant(0), x);
    </pre>
    </div>
</div>

### reshape ### {#api-neuralnetworkcontext-reshape}
Reshapes a tensor to a given new shape.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand reshape(Operand input, sequence<long> newShape);
};
</script>
<div algorithm=reshape>
    **Arguments:**
        - *input*: an {{Operand}}. The input tensor.
        - *newShape*: a sequence of {{long}}. The shape of the output tensor.
            The number of elements implied by *newShape* must be the same as the
            number of elements in the input tensor. Only one component of
            *newShape* can be the special value of -1. The size of the dimension
            with the value -1 is computed so that the total size remains
            constant.

    **Returns:** an {{Operand}}. The output tensor. The values of the output
    tensor are the same as values of the input tensor. The shape of the output
    tensor is specified by the *newShape* argument.
</div>

### slice ### {#api-neuralnetworkcontext-slice}
Produce a slice of the input tensor.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand slice(Operand input, sequence<long> starts, sequence<long> sizes,
                optional sequence<long> axes);
};
</script>
<div algorithm=slice>
    **Arguments:**
        - *input*: an {{Operand}}. The input tensor.
        - *starts*: a sequence of {{long}}. The starting indices to slice of the corresponding axes of the input shape. A negative index value is interpreted as counting back from the end. For example, the value -1 
        - *sizes*: a sequence of {{long}}. The lengths to slice of the corresponding axes of the input shape.
            The length value of -1 selects all the remaining elements from the starting index of the given axis.
        - *axes*: an optional sequence of {{long}}. The dimensions of the input shape to which *starts* and *sizes* apply. The values in the sequence are either within the [0, *r*-1] range where *r* is the input tensor rank, or the [*-r*, -1] range where negative values mean counting back from the end of the input shape. When not specified, the sequence is assumed to be [0,1,..*r-1*].  

    **Returns:** an {{Operand}}. The output tensor of the same rank as the input tensor with tensor values stripped to the specified starting and ending indices in each dimension.
</div>

### softmax ### {#api-neuralnetworkcontext-softmax}
Compute the [softmax](https://en.wikipedia.org/wiki/Softmax_function) values of
the 2-D input tensor along axis 1.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand softmax(Operand x);
};
</script>
<div algorithm=softmax>
    **Arguments:**
        - *x*: an {{Operand}}. The input 2-D tensor.

    **Returns:** an {{Operand}}. The output 2-D tensor that contains the softmax
    results, of the same shape as the input tensor.

    <div class="note">
    The behavior of this operation can be generically emulated from the usage of
    other operations as follow. However, user agents typically have a more
    efficient implementation for it, therefore its usage is encouraged from the
    performance standpoint.
    <pre highlight="js">
    // This sample deploys a well-known implementation trick [1] to compute the
    // exponentials of the distances to the max value, instead of the exponentials
    // of the input values itself, in order to increase the numerical stability of
    // the result.
    // [1]: https://cs231n.github.io/linear-classify/#softmax
    const max_x = nn.reduceMax(x, /* axes = */ [1], /* keepDimensions = */ true);
    const exp_x = nn.exp(nn.sub(x, max));
    return nn.div(exp_x, nn.reduceSum(exp_x, /* axes = */ [1], /* keepDimensions = */ true));
    </pre>
    </div>
</div>

### squeeze ### {#api-neuralnetworkcontext-squeeze}
Reduce the rank of a tensor without affecting its values by eliminating dimensions with size 1 of the tensor shape.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand squeeze(Operand input, optional sequence<long> axes);
};
</script>
<div algorithm=squeeze>
    **Arguments:**
        - *input*: an {{Operand}}. The input tensor.
        - *axes*: an optional sequence of {{long}}. Indices to the shape dimensions of size 1 to eliminate. When not specified, every shape dimensions of size 1 in the tensor are eliminated.

    **Returns:** an {{Operand}}. The output tensor of the same or reduced rank with the shape dimensions of size 1 eliminated.
</div>

### transpose ### {#api-neuralnetworkcontext-transpose}
Permute the dimensions of the input tensor according to the *permutation* argument.
<script type=idl>
partial interface NeuralNetworkContext {
  Operand transpose(Operand input, optional sequence<long> permutation);
};
</script>
<div algorithm=transpose>
    **Arguments:**
        - *input*: an {{Operand}}. The input N-D tensor.
        - *permutation*: an optional sequence of {{long}} values. The values used to permute the output shape. When it's not specified, it's set to `[N-1...0]`, where `N` is the rank of the input tensor. These default values cause the output to become a transposed tensor of the input. When specified, the number of values in the sequence must be the same as the rank of the input tensor, and the values in the sequence must be within the range from 0 to N-1 with no two or more same values found in the sequence.

    **Returns:** an {{Operand}}. The permuted or transposed N-D tensor. 
</div>

## Model ## {#api-model}
<script type=idl>
enum PowerPreference {
  // Let the user agent decide the most suitable behavior. This is the default value.
  "default",
  // Prioritizes execution speed over other considerations e.g. power consumption
  "high-performance",
  // Prioritizes power consumption over other considerations e.g. execution speed
  "low-power"
};

dictionary CompilationOptions {
  // Compilation preference as related to power consumption level
  PowerPreference powerPreference = "default";
};

interface Model {
  Promise<Compilation> createCompilation(optional CompilationOptions options = {});
};
</script>

## Compilation ## {#api-compilation}
<script type=idl>
interface Compilation {
  Promise<Execution> createExecution();
};
</script>

## Execution ## {#api-execution}
<script type=idl>
interface Execution {
  void setInput(DOMString name, ArrayBufferView data);
  void setOutput(DOMString name, ArrayBufferView data);
  Promise<void> startCompute();
};
</script>

Examples {#examples}
=====================

<div class="example">
The following code gets the NeuralNetworkContext object.
<pre highlight="js">
const nn = navigator.ml.getNeuralNetworkContext();
</pre>
</div>

<div class="example">
The following code builds a graph as:
<pre>
constant1 ---+
             +--- Add ---> intermediateOutput1 ---+
input1    ---+                                    |
                                                  +--- Mul---> output
constant2 ---+                                    |
             +--- Add ---> intermediateOutput2 ---+
input2    ---+
</pre>
<pre highlight="js">
// Use tensors in 4 dimensions.
const TENSOR_DIMS = [2, 2, 2, 2];
const TENSOR_SIZE = 16;

// Create OperandDescriptor object.
const float32TensorType = {type: 'tensor-float32', dimensions: TENSOR_DIMS};

// constant1 is a constant tensor with the value 0.5.
const constantBuffer1 = new Float32Array(TENSOR_SIZE).fill(0.5);
const constant1 = nn.constant(float32TensorType, constantBuffer1);

// input1 is one of the input tensors. Its value will be set before execution.
const input1 = nn.input('input1', float32TensorType);

// constant2 is another constant tensor with the value 0.5.
const constantBuffer2 = new Float32Array(TENSOR_SIZE).fill(0.5);
const constant2 = nn.constant(float32TensorType, constantBuffer2);

// input2 is another input tensor. Its value will be set before execution.
const input2 = nn.input('input2', float32TensorType);

// intermediateOutput1 is the output of the first Add operation.
const intermediateOutput1 = nn.add(constant1, input1);

// intermediateOutput2 is the output of the second Add operation.
const intermediateOutput2 = nn.add(constant2, input2);

// output is the output tensor of the Mul operation.
const output = nn.mul(intermediateOutput1, intermediateOutput2);

// Create the model by identifying the outputs.
const model = await nn.createModel([{name: 'output', operand: output}]);
</pre>
</div>

<div class="example">
The following code compiles the model by prioritizing lower level of power consumption
over time. This option could be particularly useful for long-running models.
<pre highlight="js">
// Create a Compilation object for the constructed model.
const options = { powerPreference: 'low-power' };
const compilation = await model.createCompilation(options);
</pre>
</div>

<div class="example">
The following code executes the compiled graph.
<pre highlight="js">
// Create an Execution object for the compiled model.
const execution = await compilation.createExecution();

// Setup the input buffers with value 1.
const inputBuffer1 = new Float32Array(TENSOR_SIZE).fill(1);
const inputBuffer2 = new Float32Array(TENSOR_SIZE).fill(1);

// Associate the input buffers to model's inputs.
execution.setInput('input1', inputBuffer1);
execution.setInput('input2', inputBuffer2);

// Associate the output buffer to model's output.
let outputBuffer = new Float32Array(TENSOR_SIZE);
execution.setOutput('output', outputBuffer);

// Start the asynchronous computation.
await execution.startCompute();
// The computed result is now in outputBuffer.
console.log(outputBuffer);
</pre>
</div>

<h2 id="acknowledgements">Acknowledgements</h2>

This specification follows the concepts of the Android Neural Networks API C
API.

Thanks to Tomoyuki Shimizu, Ningxin Hu, Zhiqiang Yu and Belem Zhang for the use
cases.

Thanks to Nikhil Thorat, Daniel Smilkov, Ganesan Ramalingam, Rafael Cintron and
Benjamin Poulain for their contributions to the API specification.

<pre class="biblio">
{
  "Models": {
      "href": "https://github.com/webmachinelearning/webnn/blob/master/op_compatibility/first_wave_models.md",
      "title": "The first-wave models",
      "authors": ["Machine Learning for the Web Community Group"],
      "date": "2020"
  },
  "numpy-broadcasting-rule": {
    "href": "https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html#general-broadcasting-rules",
    "title": "General Broadcasting Rules of NumPy",
    "authors": ["The SciPy community"],
    "date": "July 2019"
  },
  "SSD": {
    "href": "https://arxiv.org/abs/1512.02325",
    "title": "SSD: Single Shot MultiBox Detector",
    "authors": [
      "Wei Liu",
      "Dragomir Anguelov",
      "Dumitru Erhan",
      "Christian Szegedy",
      "Scott Reed",
      "Cheng-Yang Fu",
      "Alexander C. Berg"
    ],
    "date": "December 2016"
  },
  "YOLO": {
    "href": "https://arxiv.org/abs/1506.02640",
    "title": "You Only Look Once: Unified, Real-Time Object Detection",
    "authors": [
      "Joseph Redmon",
      "Santosh Divvala,",
      "Ross Girshick",
      "Ali Farhadi"
    ],
    "date": "May 2016"
  },
  "DeepLabv3+": {
    "href": "https://arxiv.org/abs/1802.02611",
    "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
    "authors": [
      "Liang-Chieh Chen",
      "Yukun Zhu",
      "George Papandreou",
      "Florian Schroff",
      "Hartwig Adam"
    ],
    "date": "August 2018"
  },
  "MaskR-CNN": {
    "href": "https://arxiv.org/abs/1703.06870",
    "title": "Mask R-CNN",
    "authors": [
      "Kaiming He",
      "Georgia Gkioxari",
      "Piotr DollÃ¡r",
      "Ross Girshick"
    ],
    "date": "January 2018"
  },
  "PoseNet": {
    "href": "https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5",
    "title": "Real-time Human Pose Estimation in the Browser with TensorFlow.js",
    "authors": [
      "Dan Oved"
    ],
    "date": "May 2018"
  },
  "FaceNet": {
    "href": "https://arxiv.org/abs/1503.03832",
    "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
    "authors": [
      "Florian Schroff",
      "Dmitry Kalenichenko",
      "James Philbin"
    ],
    "date": "June 2015"
  },
  "FAN": {
    "href": "https://arxiv.org/abs/1703.07332",
    "title": "How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)",
    "authors": [
      "Adrian Bulat",
      "Georgios Tzimiropoulos"
    ],
    "date": "September 2017"
  },
  "ContextualLoss": {
    "href": "https://arxiv.org/abs/1803.02077",
    "title": "The Contextual Loss for Image Transformation with Non-Aligned Data",
    "authors": [
      "Roey Mechrez",
      "Itamar Talmi",
      "Lihi Zelnik-Manor"
    ],
    "date": "July 2018"
  },
  "PairedCycleGAN": {
    "href": "http://openaccess.thecvf.com/content_cvpr_2018/html/Chang_PairedCycleGAN_Asymmetric_Style_CVPR_2018_paper.html",
    "title": "PairedCycleGAN: Asymmetric Style Transfer for Applying and Removing Makeup",
    "authors": [
      "Huiwen Chang",
      "Jingwan Lu",
      "Fisher Yu",
      "Adam Finkelstein"
    ],
    "date": "June 2018"
  },
  "SRGAN": {
    "href": "https://arxiv.org/abs/1609.04802",
    "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
    "authors": [
      "Christian Ledig",
      "Lucas Theis",
      "Ferenc Huszar",
      "Jose Caballero",
      "Andrew Cunningham",
      "Alejandro Acosta",
      "Andrew Aitken",
      "Alykhan Tejani",
      "Johannes Totz",
      "Zehan Wang",
      "Wenzhe Shi"
    ],
    "date": "May 2017"
  },
  "im2txt": {
    "href": "https://arxiv.org/abs/1609.06647",
    "title": "Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge",
    "authors": [
      "Oriol Vinyals",
      "Alexander Toshev",
      "Samy Bengio",
      "Dumitru Erhan"
    ],
    "date": "September 2016"
  },
  "GNMT": {
    "href": "https://github.com/tensorflow/nmt",
    "title": "Neural Machine Translation (seq2seq) Tutorial",
    "authors": [
      "Minh-Thang Luong",
      "Eugene Brevdo",
      "Rui Zhao"
    ],
    "date": "May 2017"
  },
  "OpenNMT": {
    "href": "https://arxiv.org/abs/1701.02810",
    "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
    "authors": [
      "Guillaume Klein",
      "Yoon Kim",
      "Yuntian Deng",
      "Jean Senellart",
      "Alexander M. Rush"
    ],
    "date": "March 2017"
  },
  "DeepMoji": {
    "href": "https://arxiv.org/abs/1708.00524",
    "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
    "authors": [
      "Bjarke Felbo",
      "Alan Mislove",
      "Anders SÃ¸gaard",
      "Iyad Rahwan",
      "Sune Lehmann"
    ],
    "date": "October 2017"
  },
  "Video-Summarization-with-LSTM": {
    "href": "http://www-scf.usc.edu/~zhan355/ke_eccv2016.pdf",
    "title": "Video summarization with long short-term memory",
    "authors": [
      "Ke Zhang",
      "Wei-Lun Chao",
      "Fei Sha",
      "Kristen Grauman"
    ],
    "date": "October 2016"
  },
  "LeakyReLU": {
    "href": "https://pdfs.semanticscholar.org/367f/2c63a6f6a10b3b64b8729d601e69337ee3cc.pdf",
    "title": "Rectifier Nonlinearities Improve Neural Network Acoustic Models",
    "authors": [
      "Andrew L. Maas",
      "Awni Y. Hannun",
      "Andrew Y. Ng"
    ],
    "date": "June 2013"
  },
  "ELU": {
    "href": "https://arxiv.org/abs/1511.07289",
    "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
    "authors": [
      "Djork-ArnÃ© Clevert",
      "Thomas Unterthiner",
      "Sepp Hochreiter"
    ],
    "date": "February 2016"
  },
  "RNNoise": {
    "href": "https://github.com/xiph/rnnoise",
    "title": "Recurrent neural network for audio noise reduction",
    "authors": [
      "Jean-Marc Valin"
    ],
    "date": "September 2017"
  },
  "GRU": {
    "href": "https://arxiv.org/pdf/1406.1078.pdf",
    "title": "Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation",
    "authors": [
      "Kyunghyun Cho",
      "Bart van Merrienboer",
      "Caglar Gulcehre",
      "Dzmitry Bahdanau",
      "Fethi Bougares",
      "Holger Schwenk",
      "Yoshua Bengio"
    ],
    "date": "September 2014"
  },
  "BatchNorm": {
    "href": "https://arxiv.org/abs/1502.03167",
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "authors": [
      "Sergey Ioffe",
      "Christian Szegedy"
    ],
    "date": "March 2015"
  }
}
</pre>